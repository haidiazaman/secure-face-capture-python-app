{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a584bc4-84d2-4077-b01e-d4445ff6d37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ff/wvby4kh52j96myw15h33jjk40000gn/T/ipykernel_92198/2421759533.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import glob\n",
    "import os\n",
    "# from alive_progress import alive_it\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1247eb4-310b-4b0b-804f-04e88e13bd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>subject_ID</th>\n",
       "      <th>image_number</th>\n",
       "      <th>gender</th>\n",
       "      <th>glasses</th>\n",
       "      <th>eye_state</th>\n",
       "      <th>reflections</th>\n",
       "      <th>image_quality</th>\n",
       "      <th>sensor_type</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/I748920/Desktop/secure-face-capture-pyt...</td>\n",
       "      <td>s0030</td>\n",
       "      <td>663</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/I748920/Desktop/secure-face-capture-pyt...</td>\n",
       "      <td>s0030</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/I748920/Desktop/secure-face-capture-pyt...</td>\n",
       "      <td>s0030</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/I748920/Desktop/secure-face-capture-pyt...</td>\n",
       "      <td>s0030</td>\n",
       "      <td>782</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/I748920/Desktop/secure-face-capture-pyt...</td>\n",
       "      <td>s0030</td>\n",
       "      <td>764</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84893</th>\n",
       "      <td>/Users/I748920/Desktop/secure-face-capture-pyt...</td>\n",
       "      <td>s0016</td>\n",
       "      <td>534</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>close</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84894</th>\n",
       "      <td>/Users/I748920/Desktop/secure-face-capture-pyt...</td>\n",
       "      <td>s0016</td>\n",
       "      <td>1793</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84895</th>\n",
       "      <td>/Users/I748920/Desktop/secure-face-capture-pyt...</td>\n",
       "      <td>s0016</td>\n",
       "      <td>331</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>close</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84896</th>\n",
       "      <td>/Users/I748920/Desktop/secure-face-capture-pyt...</td>\n",
       "      <td>s0016</td>\n",
       "      <td>709</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>close</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84897</th>\n",
       "      <td>/Users/I748920/Desktop/secure-face-capture-pyt...</td>\n",
       "      <td>s0016</td>\n",
       "      <td>737</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>close</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84898 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              image_path subject_ID  \\\n",
       "0      /Users/I748920/Desktop/secure-face-capture-pyt...      s0030   \n",
       "1      /Users/I748920/Desktop/secure-face-capture-pyt...      s0030   \n",
       "2      /Users/I748920/Desktop/secure-face-capture-pyt...      s0030   \n",
       "3      /Users/I748920/Desktop/secure-face-capture-pyt...      s0030   \n",
       "4      /Users/I748920/Desktop/secure-face-capture-pyt...      s0030   \n",
       "...                                                  ...        ...   \n",
       "84893  /Users/I748920/Desktop/secure-face-capture-pyt...      s0016   \n",
       "84894  /Users/I748920/Desktop/secure-face-capture-pyt...      s0016   \n",
       "84895  /Users/I748920/Desktop/secure-face-capture-pyt...      s0016   \n",
       "84896  /Users/I748920/Desktop/secure-face-capture-pyt...      s0016   \n",
       "84897  /Users/I748920/Desktop/secure-face-capture-pyt...      s0016   \n",
       "\n",
       "       image_number  gender  glasses  eye_state  reflections  image_quality  \\\n",
       "0               663       0        0          1            0              1   \n",
       "1               486       0        0          1            0              1   \n",
       "2               384       0        0          1            0              1   \n",
       "3               782       0        0          1            0              1   \n",
       "4               764       0        0          1            0              1   \n",
       "...             ...     ...      ...        ...          ...            ...   \n",
       "84893           534       1        0          0            1              1   \n",
       "84894          1793       1        0          1            0              1   \n",
       "84895           331       1        0          0            0              1   \n",
       "84896           709       1        0          0            0              1   \n",
       "84897           737       1        0          0            0              1   \n",
       "\n",
       "       sensor_type  label  \n",
       "0                1   open  \n",
       "1                1   open  \n",
       "2                1   open  \n",
       "3                1   open  \n",
       "4                1   open  \n",
       "...            ...    ...  \n",
       "84893            1  close  \n",
       "84894            1   open  \n",
       "84895            1  close  \n",
       "84896            1  close  \n",
       "84897            1  close  \n",
       "\n",
       "[84898 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/v1.0.0.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df40bb7e-5eac-4a53-a026-4fbb3fb5ab2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image_path', 'subject_ID', 'image_number', 'gender', 'glasses',\n",
       "       'eye_state', 'reflections', 'image_quality', 'sensor_type', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca944d2e-e55d-4689-b101-7e88f8cd7737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train,test = train_test_split(df,test_size=0.1,shuffle=True,random_state=1,stratify=df[['label','gender', 'glasses','reflections', 'image_quality']])\n",
    "train,val = train_test_split(train,test_size=2/7,shuffle=True,random_state=1,stratify=train[['label','gender', 'glasses','reflections', 'image_quality']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b273283-24ab-4675-9e3d-fd41ae4f732d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(label\n",
       " open     0.505927\n",
       " close    0.494073\n",
       " Name: count, dtype: float64,\n",
       " label\n",
       " open     0.505932\n",
       " close    0.494068\n",
       " Name: count, dtype: float64,\n",
       " label\n",
       " open     0.505889\n",
       " close    0.494111\n",
       " Name: count, dtype: float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train),len(val),len(test)\n",
    "\n",
    "train.label.value_counts()/len(train),val.label.value_counts()/len(val),test.label.value_counts()/len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9ea76116-ee0e-48a2-abac-eaf14d623323",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:23, 421.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find what the min_dim for the dataset is, that should resize all images to a number close to that, so that small images are not upsampled to larger dim instead\n",
    "min_dim = np.float64(\"inf\")\n",
    "for i,row in tqdm(df.sample(n=10000).iterrows()):\n",
    "    dims = plt.imread(row['image_path']).shape\n",
    "    dims = min(dims)\n",
    "    if dims<min_dim:\n",
    "        min_dim=dims\n",
    "\n",
    "min_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd5045e-06a8-41fc-9d2f-8f715c6f1da8",
   "metadata": {},
   "source": [
    "should use a try except function and read all the files to ensure that all images are actually readable and not corrupt, should also set a threshold on min_dims so that weird images that are too small arent used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ba420d86-dcae-43a5-bf6d-f84f15ae0385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose,Resize,ToTensor\n",
    "\n",
    "def load_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    return img\n",
    "    \n",
    "\n",
    "class EyeDataset(Dataset):\n",
    "    def __init__(self,df,split_type):\n",
    "        self.split_type = split_type\n",
    "\n",
    "        self.image_paths = df.image_path.tolist()\n",
    "        self.labels = df.label.tolist()\n",
    "        self.mapping = {\n",
    "            \"close\":0,\n",
    "            \"open\":1,\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)       \n",
    "        \n",
    "    def __getitem__(self,ind):\n",
    "        image = load_image(self.image_paths[ind])\n",
    "        label = self.mapping[self.labels[ind]]\n",
    "\n",
    "        if self.split_type=='train':\n",
    "            # apply train transforms\n",
    "            pass\n",
    "        else:\n",
    "            # apply val and test transforms, resize, normalise, totensor\n",
    "            val_transforms = Compose([\n",
    "                ToTensor(), # converts to type torch tensor and normalise to [0,1]\n",
    "                Resize((50,50))\n",
    "            ])\n",
    "            image = val_transforms(image)\n",
    "       \n",
    "        \n",
    "        return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5c5752a3-b723-4680-87c2-698d747b67f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 50])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset = EyeDataset(\n",
    "    df=train,\n",
    "    split_type='val'\n",
    ")\n",
    "\n",
    "sample_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d851c90b-a8de-4fec-9cee-a125b4163770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsa5205-env",
   "language": "python",
   "name": "dsa5205-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
