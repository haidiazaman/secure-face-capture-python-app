{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e885fb5-d4b8-4748-9a36-ba39d33bb855",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from eye_dataset_v1 import *\n",
    "from eye_model_v1 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55f312a0-d00a-434d-9ab3-acf79eabcc44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "\n",
    "# generate mouth boxes\n",
    "def get_eye_box(face_bbox,face_landmarks,multiplier=3):\n",
    "    '''\n",
    "    this function creates the mouth bbox for the face image by using the landmarks of the mouth corners. If you are not using the RetinaFace landmarks, then you need to replace \n",
    "    face_landmarks[6:8],face_landmarks[8:10] with the landmarks corresponding to the left corner and right corner respectively.\n",
    "    '''\n",
    "    l,t,r,b=face_bbox\n",
    "    face_landmarks=np.array(face_landmarks).reshape(5,2)\n",
    "    left_eye_landmark=face_landmarks[0]\n",
    "    lx,ly=left_eye_landmark\n",
    "    right_eye_landmark=face_landmarks[1]\n",
    "    rx,ry=right_eye_landmark\n",
    "    eye_box_width = (rx-lx)/multiplier\n",
    "    eye_box_height = eye_box_width\n",
    "    left_l,left_t,left_r,left_b=lx-eye_box_width*0.5,ly-eye_box_height*0.5,lx+eye_box_width*0.5,ly+eye_box_height*0.5\n",
    "    right_l,right_t,right_r,right_b=rx-eye_box_width*0.5,ry-eye_box_height*0.5,rx+eye_box_width*0.5,ry+eye_box_height*0.5    \n",
    "    left_eye_bbox = [int(left_l),int(left_t),int(left_r),int(left_b)]\n",
    "    right_eye_bbox = [int(right_l),int(right_t),int(right_r),int(right_b)]\n",
    "    return left_eye_bbox,right_eye_bbox\n",
    "\n",
    "\n",
    "def load_image(image_path,bbox,input_dim = 80,expand_ratio = 0):\n",
    "    '''\n",
    "    this function reads the image using cv2, crops the image according to the bbox given. It does clipping to ensure image crop is valid and then resizes to the input dim. \n",
    "    You can also pass the expand ratio argument to expand the bbox.\n",
    "    '''\n",
    "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "    l, t, r, b = bbox\n",
    "    w, h = r - l, b - t\n",
    "    l, r = max(0, l - int(expand_ratio * w / 2)), min(image.shape[1] - 1, r + int(expand_ratio * w / 2))\n",
    "    t, b = max(0, t - int(expand_ratio * h / 2)), min(image.shape[0] - 1, b + int(expand_ratio * h / 2))\n",
    "    image = cv2.resize(image[t:b, l:r], (input_dim, input_dim))\n",
    "    return image\n",
    "\n",
    "    \n",
    "def apply_input_transforms(image): \n",
    "    '''\n",
    "    this functions applies /255 and ToTensor to convert a numpy array to torch.tensor and normalises it.\n",
    "    '''\n",
    "    transforms=Compose([\n",
    "        ToTensor()\n",
    "    ])\n",
    "    image=transforms(image)\n",
    "    return image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f41c1dd2-2d5b-4440-90fb-ccc5191c3b6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/data/aurora/ISO_L2_eye_mouth_blink/eye/train/result/v3.3.4.1/weights/epoch987.pt\n"
     ]
    }
   ],
   "source": [
    "# inputs\n",
    "\n",
    "multiplier = 1.5\n",
    "\n",
    "model_key = 'mobilenetv3_small_050'\n",
    "output_folder = 'v3.3.4.1'\n",
    "epoch = 987\n",
    "model_path = f'/home/jovyan/data/aurora/ISO_L2_eye_mouth_blink/eye/train/result/{output_folder}/weights/epoch{epoch}.pt' \n",
    "print(model_path)\n",
    "input_dim = 80\n",
    "in_channel = 3\n",
    "expand_ratio = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45f97045-2b81-43f2-b506-a95cfb860dfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input a list of images and their face landmarks - e.g. get landmarks using RetinaFace package\n",
    "\n",
    "csvPath = f'/home/jovyan/data/aurora/ISO_L2_eye_mouth_blink/mouth/data/v1.2/v1.2.2_test.csv' \n",
    "\n",
    "df = pd.read_csv(csvPath)\n",
    "df = df[~df.rf_landmarks.isna()]\n",
    "image_paths = df.image_path.tolist()\n",
    "bboxes = [eval(x) for x in df.rf_bbox.tolist()]\n",
    "landmarks = [eval(x) for x in df.rf_landmarks.tolist()]\n",
    "\n",
    "# create eye boxes from the image landmarks\n",
    "eye_bboxes = [get_eye_box(bbox,landmark,multiplier) for bbox,landmark  in zip(bboxes,landmarks)] # contains both left and right eye bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36320531-3744-4a73-9e09-34c6877b8b80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded succesfully\n"
     ]
    }
   ],
   "source": [
    "# initialise model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# setup model - from scratch or load model for finetune\n",
    "model = Eye_Net(\n",
    "    model_key = model_key,\n",
    "    in_channel = in_channel\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(os.path.join(model_path), map_location = device))\n",
    "_ = model.eval()\n",
    "print('model loaded succesfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2212b46-e6e2-4816-a247-1d15b94916c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1299it [01:03, 20.53it/s]\n"
     ]
    }
   ],
   "source": [
    "left_scores,right_scores = [],[]\n",
    "\n",
    "for image_path,bbox in tqdm(zip(image_paths,eye_bboxes)):\n",
    "    left_bbox,right_bbox = bbox[0],bbox[1]\n",
    "    left_image = load_image(image_path,left_bbox,input_dim,expand_ratio)\n",
    "    right_image = load_image(image_path,right_bbox,input_dim,expand_ratio)\n",
    "    left_image = apply_input_transforms(left_image)\n",
    "    right_image = apply_input_transforms(right_image)\n",
    "    \n",
    "    left_logits = model(left_image.unsqueeze(0).cuda())\n",
    "    left_logits = left_logits.cpu().detach()\n",
    "    left_softmax_output = nn.Softmax(dim=1)(left_logits)\n",
    "    left_softmax_output = [round(x,5) for x in left_softmax_output.numpy()[0].tolist()]\n",
    "    left_scores.append(left_softmax_output)\n",
    "    \n",
    "    right_logits = model(right_image.unsqueeze(0).cuda())\n",
    "    right_logits = right_logits.cpu().detach()\n",
    "    right_softmax_output = nn.Softmax(dim=1)(right_logits)\n",
    "    right_softmax_output = [round(x,5) for x in right_softmax_output.numpy()[0].tolist()]\n",
    "    right_scores.append(right_softmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac1f4b60-2b83-4e69-834f-dfde1d4550e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1299, 1299, 1299, 1299)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_paths),len(eye_bboxes),len(left_scores),len(right_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10048d92-8b86-4483-b840-849ed6eb4618",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3k/vv9sv0rd0453t8myv393d4mr0000gn/T/ipykernel_29136/2332026345.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mleft_bbox\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mright_bbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meye_bboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meye_bboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mleft_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mleft_bbox\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexpand_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_paths' is not defined"
     ]
    }
   ],
   "source": [
    "mapping = {\n",
    "    0:'open',\n",
    "    1:'close',\n",
    "    2:'block'\n",
    "}\n",
    "\n",
    "for i in range(10):\n",
    "    image_path = image_paths[i]\n",
    "    left_bbox,right_bbox = eye_bboxes[i][0],eye_bboxes[i][1]\n",
    "    left_image = load_image(image_path,left_bbox,input_dim,expand_ratio)\n",
    "    right_image = load_image(image_path,right_bbox,input_dim,expand_ratio)\n",
    "    left_score,right_score = left_scores[i],right_scores[i]\n",
    "    \n",
    "    left_argmax_prediction,right_argmax_prediction = mapping[np.argmax(left_score)],mapping[np.argmax(right_score)]\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(left_image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'P:{left_argmax_prediction}\\n{left_score}')\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(right_image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'P:{right_argmax_prediction}\\n{right_score}')\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsa5204",
   "language": "python",
   "name": "dsa5204"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
